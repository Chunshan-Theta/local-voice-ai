# Ollama API endpoint - customize this to your external Ollama service
OLLAMA_BASE_URL=https://site.ollama.lazyinwork.com

# Ollama model configuration
# 常用模型: llama3, gemma3:4b, qwen2.5:3b 等
OLLAMA_MODEL=gemma3:4b

# Whisper model configuration (tiny/base/small/medium/large)
# tiny: 最快但準確度較低
# small: 平衡速度和準確度 (推薦)
# medium: 更高準確度但較慢
# large: 最高準確度但最慢
WHISPER_MODEL=small

# Next.js configuration
NODE_ENV=production 