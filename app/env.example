# Ollama API endpoint - 請替換為您的 Ollama 服務地址
# 本地開發: http://localhost:11434
# 生產環境: 請設置為您的實際服務地址
OLLAMA_BASE_URL=

# Ollama model configuration
# 常用模型: llama3, gemma3:4b, qwen2.5:3b 等
OLLAMA_MODEL=gemma3:12b

# Whisper model configuration (tiny/base/small/medium/large)
# tiny: 最快但準確度較低
# small: 平衡速度和準確度 (推薦)
# medium: 更高準確度但較慢
# large: 最高準確度但最慢
WHISPER_MODEL=small

# Whisper service URL - 內部服務地址
WHISPER_SERVICE_URL=

# TTS
TTS_SERVICE_URL=
