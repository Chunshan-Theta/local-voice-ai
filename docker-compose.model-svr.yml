version: '3.3'

services:
  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: model-svr-nginx
    ports:
      - "8000:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - model-svr-network
    restart: unless-stopped
    depends_on:
      - ollama-service
      - whisper-service
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama LLM Service
  ollama-service:
    image: ollama/ollama:latest
    container_name: model-svr-ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/ollama/models
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - ollama-storage:/ollama
    networks:
      - model-svr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]


  # Whisper Speech-to-Text Service
  whisper-service:
    build:
      context: ./whisper-service
      dockerfile: Dockerfile
    container_name: model-svr-whisper
    environment:
      - HF_HOME=/cache
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=1
      - WHISPER_MODEL=medium
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    volumes:
      - hf-cache:/cache
      - ./whisper-service/tmp:/app/tmp
    networks:
      - model-svr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/status"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]


networks:
  model-svr-network:
    driver: bridge

volumes:
  # Persistent volume for Hugging Face cache (shared by BreezyVoice and Whisper)
  hf-cache:
    driver: local

  # Persistent volume for Ollama models
  ollama-storage:
    driver: local
